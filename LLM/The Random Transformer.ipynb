{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7c2a5c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b8d285e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WK1 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WV1 = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WQ1 = np.array([[0, 0, 0], [1, 1, 0], [0, 0, 1], [1, 0, 0]])\n",
    "\n",
    "WK2 = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 1, 0]])\n",
    "WV2 = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1], [1, 0, 0]])\n",
    "WQ2 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f5756b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.  , 8.  , 4.  ],\n",
       "       [6.84, 9.99, 6.84]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = np.array([[1, 3, 3, 5], [2.84, 3.99, 4, 6]])\n",
    "K1 = embedding @ WK1\n",
    "K1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "223299ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.  , 6.  , 4.  ],\n",
       "       [7.99, 8.84, 6.84]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1 = embedding @ WV1\n",
    "V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "01a7c7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.  , 3.  , 3.  ],\n",
       "       [9.99, 3.99, 4.  ]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = embedding @ WQ1\n",
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f44ab523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 68.    , 105.21  ],\n",
       "       [ 87.88  , 135.5517]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = Q1 @ K1.T\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5c283037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.2598183 , 60.74302182],\n",
       "       [50.73754166, 78.26081048]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = scores1 / np.sqrt(3)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a2e89eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.67695573e-10, 1.00000000e+00],\n",
       "       [1.11377182e-12, 1.00000000e+00]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    x = np.array(x)  # Convert input to numpy array\n",
    "    if len(x.shape) == 1:  # Handle 1D arrays by adding a new axis\n",
    "        x = x[np.newaxis, :]\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "scores1 = softmax(scores1)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8ccbe6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.99, 8.84, 6.84],\n",
       "       [7.99, 8.84, 6.84]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = scores1 @ V1\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "664afe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(x, WQ, WK, WV):\n",
    "    K = x @ WK\n",
    "    V = x @ WV\n",
    "    Q = x @ WQ\n",
    "\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(3)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a676e761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.99, 8.84, 6.84],\n",
       "       [7.99, 8.84, 6.84]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = attention(embedding, WQ1, WK1, WV1)\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0e819d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.84, 3.99, 7.99],\n",
       "       [8.84, 3.99, 7.99]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention2 = attention(embedding, WQ2, WK2, WV2)\n",
    "attention2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7dd9b56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.99, 8.84, 6.84, 8.84, 3.99, 7.99],\n",
       "       [7.99, 8.84, 6.84, 8.84, 3.99, 7.99]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = np.concatenate([attention1, attention2], axis=1)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5caaa8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.95481735, -14.12627891, -12.49250332, -18.50804518],\n",
       "       [ 11.95481735, -14.12627891, -12.49250332, -18.50804518]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just some random values\n",
    "W = np.array(\n",
    "    [\n",
    "        [0.79445237, 0.1081456, 0.27411536, 0.78394531],\n",
    "        [0.29081936, -0.36187258, -0.32312791, -0.48530339],\n",
    "        [-0.36702934, -0.76471963, -0.88058366, -1.73713022],\n",
    "        [-0.02305587, -0.64315981, -0.68306653, -1.25393866],\n",
    "        [0.29077448, -0.04121674, 0.01509932, 0.13149906],\n",
    "        [0.57451867, -0.08895355, 0.02190485, 0.24535932],\n",
    "    ]\n",
    ")\n",
    "Z = attentions @ W\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bb537ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(4, 8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1ac56f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def feed_forward(Z, W1, b1, W2, b2):\n",
    "    return relu(Z.dot(W1) + b1).dot(W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d8020ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 51.77304821,   2.26545466, -14.22306709, -14.92134175],\n",
       "       [ 51.77304821,   2.26545467, -14.22306709, -14.92134175]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = feed_forward(Z, W1, b1, W2, b2)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2a4352bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding = 4\n",
    "d_key = d_value = d_query = 3\n",
    "d_feed_forward = 8\n",
    "n_attention_heads = 2\n",
    "\n",
    "def attention(x, WQ, WK, WV):\n",
    "    K = x @ WK\n",
    "    V = x @ WV\n",
    "    Q = x @ WQ\n",
    "\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(d_key)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores\n",
    "\n",
    "def multi_head_attention(x, WQs, WKs, WVs):\n",
    "    attentions = np.concatenate(\n",
    "        [attention(x, WQ, WK, WV) for WQ, WK, WV in zip(WQs, WKs, WVs)], axis=1\n",
    "    )\n",
    "    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n",
    "    return attentions @ W\n",
    "\n",
    "def feed_forward(Z, W1, b1, W2, b2):\n",
    "    return relu(Z.dot(W1) + b1).dot(W2) + b2\n",
    "\n",
    "def encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):\n",
    "    Z = multi_head_attention(x, WQs, WKs, WVs)\n",
    "    Z = feed_forward(Z, W1, b1, W2, b2)\n",
    "    return Z\n",
    "\n",
    "def random_encoder_block(x):\n",
    "    WQs = [\n",
    "        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WKs = [\n",
    "        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WVs = [\n",
    "        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    W1 = np.random.randn(d_embedding, d_feed_forward)\n",
    "    b1 = np.random.randn(d_feed_forward)\n",
    "    W2 = np.random.randn(d_feed_forward, d_embedding)\n",
    "    b2 = np.random.randn(d_embedding)\n",
    "    return encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "748d4283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 3.  , 3.  , 5.  ],\n",
       "       [2.84, 3.99, 4.  , 6.  ]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0f09468c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.15837065, -30.64037675,  -7.86182274,  -3.35033514],\n",
       "       [  6.15840883, -30.64053491,  -7.86186341,  -3.35035016]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_encoder_block(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "683eb745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6457673.25413776, -43390583.29903263,  19039272.34502068,\n",
       "        -30805080.65915656],\n",
       "       [  6457673.25413776, -43390583.29903263,  19039272.34502068,\n",
       "        -30805080.65915656]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encoder(x, n=6):\n",
    "    for _ in range(n):\n",
    "        x = random_encoder_block(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "encoder(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fb21b3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.29300251],\n",
       "       [-4.08550252]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embedding + Z).mean(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9620cec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.63170702],\n",
       "       [10.99362604]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embedding + Z).std(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "74843210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, epsilon=1e-6):\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    std = x.std(axis=-1, keepdims=True)\n",
    "    return (x - mean) / (std + epsilon)\n",
    "\n",
    "def encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):\n",
    "    Z = multi_head_attention(x, WQs, WKs, WVs)\n",
    "    Z = layer_norm(Z + x)\n",
    "\n",
    "    output = feed_forward(Z, W1, b1, W2, b2)\n",
    "    return layer_norm(output + Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a26cc396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.71635826, -0.54866785, -0.39499776, -0.77269265],\n",
       "       [ 1.7173877 , -0.55038945, -0.40086868, -0.76612956]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm(Z + embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "276d1131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21283866, -1.5675127 ,  0.13880594,  1.2158681 ],\n",
       "       [ 0.21283514, -1.56752293,  0.13883565,  1.21585214]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encoder(x, n=6):\n",
    "    for _ in range(n):\n",
    "        x = random_encoder_block(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "encoder(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ab1dd1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.58130419,  2.1213747 , -2.65274035,  2.525234  ]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_embedding = 4\n",
    "n_attention_heads = 2\n",
    "\n",
    "E = np.array([[1, 1, 0, 1]])\n",
    "WQs = [np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)]\n",
    "WKs = [np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)]\n",
    "WVs = [np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)]\n",
    "\n",
    "Z_self_attention = multi_head_attention(E, WQs, WKs, WVs)\n",
    "Z_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "949d3ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6414116,  0.4665955, -1.7281068,  0.6200997]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_self_attention = layer_norm(Z_self_attention + E)\n",
    "Z_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bdf92f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):\n",
    "    # The next three lines are the key difference!\n",
    "    K = encoder_output @ WK    # Note that now we pass the previous encoder output!\n",
    "    V = encoder_output @ WV    # Note that now we pass the previous encoder output!\n",
    "    Q = attention_input @ WQ   # Same as self-attention\n",
    "\n",
    "    # This stays the same\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(d_key)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores\n",
    "\n",
    "\n",
    "def multi_head_encoder_decoder_attention(\n",
    "    encoder_output, attention_input, WQs, WKs, WVs\n",
    "):\n",
    "    # Note that now we pass the previous encoder output!\n",
    "    attentions = np.concatenate(\n",
    "        [\n",
    "            encoder_decoder_attention(\n",
    "                encoder_output, attention_input, WQ, WK, WV\n",
    "            )\n",
    "            for WQ, WK, WV in zip(WQs, WKs, WVs)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n",
    "    return attentions @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cf619354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.02529108, -4.32320118,  4.78989449, -9.23399638]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WQs = [np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)]\n",
    "WKs = [np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)]\n",
    "WVs = [np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)]\n",
    "\n",
    "encoder_output = np.array([[-1.5, 1.0, -0.8, 1.5], [1.0, -1.0, -0.5, 1.0]])\n",
    "\n",
    "Z_encoder_decoder = multi_head_encoder_decoder_attention(\n",
    "    encoder_output, Z_self_attention, WQs, WKs, WVs\n",
    ")\n",
    "Z_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c55026b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47793213, -0.32609457,  1.27569396, -1.42753153]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z_self_attention)\n",
    "Z_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0439251d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.66783935,  0.12313221,  0.70133706,  0.84337007]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = np.random.randn(4, 8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(4)\n",
    "\n",
    "output = layer_norm(feed_forward(Z_encoder_decoder, W1, b1, W2, b2) + Z_encoder_decoder)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "15a6b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding = 4\n",
    "d_key = d_value = d_query = 3\n",
    "d_feed_forward = 8\n",
    "n_attention_heads = 2\n",
    "encoder_output = np.array([[-1.5, 1.0, -0.8, 1.5], [1.0, -1.0, -0.5, 1.0]])\n",
    "\n",
    "def decoder_block(\n",
    "    x,\n",
    "    encoder_output,\n",
    "    WQs_self_attention, WKs_self_attention, WVs_self_attention,\n",
    "    WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,\n",
    "    W1, b1, W2, b2,\n",
    "):\n",
    "    # Same as before\n",
    "    Z = multi_head_attention(\n",
    "        x, WQs_self_attention, WKs_self_attention, WVs_self_attention\n",
    "    )\n",
    "    Z = layer_norm(Z + x)\n",
    "\n",
    "    # The next three lines are the key difference!\n",
    "    Z_encoder_decoder = multi_head_encoder_decoder_attention(\n",
    "        encoder_output, Z, WQs_ed_attention, WKs_ed_attention, WVs_ed_attention\n",
    "    )\n",
    "    Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z)\n",
    "\n",
    "    # Same as before\n",
    "    output = feed_forward(Z_encoder_decoder, W1, b1, W2, b2)\n",
    "    return layer_norm(output + Z_encoder_decoder)\n",
    "\n",
    "def random_decoder_block(x, encoder_output):\n",
    "    # Just a bunch of random initializations\n",
    "    WQs_self_attention = [\n",
    "        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WKs_self_attention = [\n",
    "        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WVs_self_attention = [\n",
    "        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "\n",
    "    WQs_ed_attention = [\n",
    "        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WKs_ed_attention = [\n",
    "        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WVs_ed_attention = [\n",
    "        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "\n",
    "    W1 = np.random.randn(d_embedding, d_feed_forward)\n",
    "    b1 = np.random.randn(d_feed_forward)\n",
    "    W2 = np.random.randn(d_feed_forward, d_embedding)\n",
    "    b2 = np.random.randn(d_embedding)\n",
    "\n",
    "\n",
    "    return decoder_block(\n",
    "        x, encoder_output,\n",
    "        WQs_self_attention, WKs_self_attention, WVs_self_attention,\n",
    "        WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,\n",
    "        W1, b1, W2, b2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "db2cf11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.58272481, -0.90031757,  1.67697472, -0.19393235]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decoder(x, decoder_embedding, n=6):\n",
    "    for _ in range(n):\n",
    "        x = random_decoder_block(x, decoder_embedding)\n",
    "    return x\n",
    "\n",
    "decoder(E, encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "56fc311d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.55368201,  3.26190882, -0.09717209,  0.42648578,  0.30394799,\n",
       "        0.97572017, -0.28418015, -1.65858315,  1.32533095, -1.06461042])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear(x, W, b):\n",
    "    return np.dot(x, W) + b\n",
    "\n",
    "x = linear([1, 0, 1, 0], np.random.randn(4, 10), np.random.randn(10))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3ded80cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00206484, 0.69273079, 0.02408431, 0.04065893, 0.03596983,\n",
       "        0.07041827, 0.01997642, 0.00505385, 0.0998894 , 0.00915337]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "542a0843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': array([-0.3740027 , -0.27331461,  0.83044974, -0.89449293]),\n",
       " 'mundo': array([ 0.51187159, -0.23632051,  1.36690286, -0.48006702]),\n",
       " 'world': array([ 1.56589079, -0.62575696, -1.28642105, -0.6150428 ]),\n",
       " 'how': array([ 1.5089655 ,  0.48213663, -0.93913669,  1.87220858]),\n",
       " '?': array([-0.40593975, -1.48501512,  1.24030524, -1.16183631]),\n",
       " 'EOS': array([-1.28896434,  0.34225844, -0.27898599, -1.92970403]),\n",
       " 'SOS': array([-0.50054203, -0.29423137, -1.40294651, -0.12970428]),\n",
       " 'a': array([ 1.1007179 , -1.86644389, -0.81330588,  0.81295671]),\n",
       " 'hola': array([ 1.2931899 ,  1.51516573, -0.11730997,  0.86117891]),\n",
       " 'c': array([ 1.76754676, -1.49698142,  2.94760718,  0.04001005])}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = [\n",
    "    \"hello\",\n",
    "    \"mundo\",\n",
    "    \"world\",\n",
    "    \"how\",\n",
    "    \"?\",\n",
    "    \"EOS\",\n",
    "    \"SOS\",\n",
    "    \"a\",\n",
    "    \"hola\",\n",
    "    \"c\",\n",
    "]\n",
    "embedding_reps = np.random.randn(10, 4)\n",
    "vocabulary_embeddings = {\n",
    "    word: embedding_reps[i] for i, word in enumerate(vocabulary)\n",
    "}\n",
    "vocabulary_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "88522c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_sequence, max_iters=3):\n",
    "    # We first encode the inputs into embeddings\n",
    "    # This skips the positional encoding step for simplicity\n",
    "    embedded_inputs = [\n",
    "        vocabulary_embeddings[token] for token in input_sequence\n",
    "    ]\n",
    "    print(\"Embedding representation (encoder input)\", embedded_inputs)\n",
    "\n",
    "    # We then generate an embedding representation\n",
    "    encoder_output = encoder(embedded_inputs)\n",
    "    print(\"Embedding generated by encoder (encoder output)\", encoder_output)\n",
    "\n",
    "    # We initialize the decoder output with the embedding of the start token\n",
    "    sequence_embeddings = [vocabulary_embeddings[\"SOS\"]]\n",
    "    output = \"SOS\"\n",
    "    \n",
    "    # Random matrices for the linear layer\n",
    "    W_linear = np.random.randn(d_embedding, len(vocabulary))\n",
    "    b_linear = np.random.randn(len(vocabulary))\n",
    "\n",
    "    # We limit number of decoding steps to avoid too long sequences without EOS\n",
    "    for i in range(max_iters):\n",
    "        # Decoder step\n",
    "        decoder_output = decoder(sequence_embeddings, encoder_output)\n",
    "\n",
    "        # Only use the last output for prediction\n",
    "        logits = linear(decoder_output[-1], W_linear, b_linear)\n",
    "        # We wrap logits in a list as our softmax expects batches/2D array\n",
    "        probs = softmax([logits])\n",
    "\n",
    "        # We get the most likely next token\n",
    "        next_token = vocabulary[np.argmax(probs)]\n",
    "        sequence_embeddings.append(vocabulary_embeddings[next_token])\n",
    "        output += \" \" + next_token\n",
    "\n",
    "        print(\n",
    "            \"Iteration\", i, \n",
    "            \"next token\", next_token,\n",
    "            \"with probability of\", np.max(probs),\n",
    "        )\n",
    "\n",
    "        # If the next token is the end token, we return the sequence\n",
    "        if next_token == \"EOS\":\n",
    "            return output\n",
    "\n",
    "    return output, sequence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ec8d34f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding representation (encoder input) [array([-0.3740027 , -0.27331461,  0.83044974, -0.89449293]), array([ 1.56589079, -0.62575696, -1.28642105, -0.6150428 ])]\n",
      "Embedding generated by encoder (encoder output) [[-1.05509726 -0.42878511  1.63691548 -0.15303311]\n",
      " [-1.0551545  -0.42873063  1.63689502 -0.15300989]]\n",
      "Iteration 0 next token SOS with probability of 0.6542590025392748\n",
      "Iteration 1 next token how with probability of 0.6583692092399231\n",
      "Iteration 2 next token a with probability of 0.41477578261860854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SOS SOS how a',\n",
       " [array([-0.50054203, -0.29423137, -1.40294651, -0.12970428]),\n",
       "  array([-0.50054203, -0.29423137, -1.40294651, -0.12970428]),\n",
       "  array([ 1.5089655 ,  0.48213663, -0.93913669,  1.87220858]),\n",
       "  array([ 1.1007179 , -1.86644389, -0.81330588,  0.81295671])])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate([\"hello\", \"world\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
